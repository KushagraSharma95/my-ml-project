---
title: "EPL Match Outcome Prediction"
summary: "EPL Match Outcome Prediction (StatsBomb Dataset) Problem Description and Business Context"
date: 2025-08-03
tags: [machine learning, portfolio]
---

# EPL Match Outcome Prediction

EPL Match Outcome Prediction (StatsBomb Dataset) Problem Description and Business Context

Predicting the outcome of football matches (win, draw, or loss) is a classic sports analytics problem with applications in betting markets, team strategy, and fan engagement. For this project, we focused on the English Premier League (EPL) and attempted to predict match results using rich event-level data from StatsBomb, a leading football (soccer) analytics provider. The business context here could be a betting company looking to set odds, or a club analyst trying to assess win probabilities given team performance metrics. Accurate predictions can potentially beat betting odds or provide competitive insights, though football outcomes are notoriously difficult to forecast due to the sport's complexity and variance. Our goal: given historical match data (teams' performances, playing styles, and match events), predict whether the home team wins, the match draws, or the away team wins. This is a multi-class classification task (three outcomes) influenced by myriad factors (team strength, player skill, tactics, luck). The EPL, in particular, has a well-known home advantage and disparities in team quality, which any model must account for. Success isn't just about overall accuracy, but also properly handling the class imbalance (home wins are most common, draws less so). From a business view, even a modest improvement over naive predictions can be valuable – for instance, a betting firm could improve odds- making, or a team analyst might identify key predictors of tough games. Data Source and Exploration We utilized the StatsBomb Open Data for soccer, which includes detailed event logs for matches. Specifically, we gathered data for multiple seasons of the EPL (and potentially other leagues for more training data, though focusing on EPL for evaluation). The data includes every on-ball event in each match (passes, shots, dribbles, tackles, etc.) along with contextual information like event coordinates and outcome. We aggregated this low-level data into match-level features.

Data Ingestion: StatsBomb provides data in JSON format (via their open data repository). We wrote a parser to extract key information for each match: Match metadata: home team, away team, competition, season, final score, and result. Event statistics: Using the event stream, we computed summary stats per team per match. For example: total shots, shots on target, possession percentage, pass completion rate, average xG (expected goals) per shot, number of carries into the final third, defensive clearances, etc. We were inspired by features used in sports analytics research. We ended up with about 40 features per team per match (we then typically take differences or ratios for home vs away). Sample Size: We collected data for ~1,800 matches (roughly 5 seasons of EPL, each with 380 matches). After parsing and feature engineering (discussed below), each match is one sample. We used one-hot encoding for categorical like team identity if needed, but in many models we excluded team IDs to generalize across teams. Exploratory Data Analysis: We first looked at the distribution of match outcomes. In our dataset, approximately 45–50% of matches were home wins, ~25% draws, ~25–30% away wins, confirming the typical home advantage effect. We visualized this in a bar chart to illustrate class imbalance. Next, we performed correlation analysis on the engineered features against outcomes. For instance, we found: Matches where the home team had higher total shots and higher expected goals (xG) often resulted in home wins. Home team's xG had a positive correlation with winning (Pearson r ~0.4). 

Conversely, if the away team had more shots or possession, the likelihood of a home win decreased (and chance of away win increased). Possession and passing metrics: A higher possession percentage by the home team correlated with winning (teams dominating the ball at home tended to win more). We computed a "field tilt" feature (share of final-third passes that were by the home team ) – this was strongly correlated with home wins as well, correlation ~0.5 in our data. Defensive actions: Interesting insights included that matches where the home team had to make an unusually high number of clearances or tackles often ended in draws or losses, indicating they were under pressure. In contrast, when the away team was forced into many defensive actions, the home side often won. We visualized a few of these relationships. One notable visualization was a correlation matrix heatmap of selected features versus outcome (win/draw/loss) labels. This showed, for example, home team's expected threat (xT, a metric of offensive threat) had a strong positive correlation with a home win outcome (correlation ~0.52). Similarly, away team's shots and xG had positive correlation with away wins. These insights align with intuition: create more threat, you likely win; endure more threat, you likely lose. Another EDA step: we examined feature distributions for each outcome class. For example, in matches the home team won, they averaged 1.8 xG vs 0.9 xG for the away team; in drawn matches, both had roughly ~1.2 xG. Visualizing these differences (e.g., box plots of xG by result) helped confirm that our features carried signal. Feature Engineering and Preprocessing From the raw event data, we engineered a rich set of features inspired by soccer analytics research. We grouped them into categories: 

General Match Stats: Possession percentage for home and away, total number of events (a proxy for game tempo), passes attempted and completed for each team, pass completion rate, number of attacks (entries into final third or penalty box). Passing Metrics: We computed features like passes into final third, crosses, through balls, etc., for each team. We also included field tilt – the percentage of final-third passes belonging to the home team, reflecting territorial dominance. Shooting Metrics: Total shots, shots on target, and cumulative Expected Goals (xG) for each team. StatsBomb data provides xG for each shot; we summed them to get a measure of chance quality created by each side. We also looked at shot efficiency (goals/shots, though goals are outcome so used carefully to avoid leakage). Dribble/Carry Metrics: Number of dribbles and carries by each team, including how many carries into the attacking third. Also, StatsBomb's Expected Threat (xT) from carries and passes: we accumulated xT values (which measure how each action increases the probability of scoring) for each team. Total xT for home vs away turned out quite informative – if one team had a significantly higher total xT, that often presaged a win for them. Defensive Metrics: Tackles, interceptions, clearances by each team. A high number of clearances by the home team might indicate they were under siege (negative sign for them), whereas high clearances by away suggests the home team was pressing strongly. Last 5 Match Form: We also appended a few features about team form: e.g., home team's points in last 5 matches, away team's goal difference in last 5, etc. This required historical data beyond the match at hand. We included these to capture team quality and momentum, which pure match event stats might not fully capture. (These were pulled from season tables; form is not part of event data but a separate data integration we did.) 

Categorical Features: We considered team identity and venue, but using team IDs directly can overfit (the model might just learn "Team X always wins"). Instead, we baked team strength into the form features above. We also one-hot encoded whether the match is a derby (rivalry game), though this was more experimental. After feature generation, we had to preprocess for modeling: We merged the home and away features into a single row per match. For many features, we took the difference (home minus away) or ratio (home/away) so that the model inherently captures the comparative strength. For example, instead of home_shots and away_shots separately, we used shots_diff = home_shots - away_shots. This reduces dimensionality and aligns with the notion that relative performance matters more. Scaling: Many features are counts (e.g., number of passes) which vary by match length and style. We normalized some features by total events or per 90 minutes. For instance, possession is inherently a percentage. xG and xT were left in absolute terms, but since those correlate with number of attacks, that's acceptable. We also did a z-score standardization for continuous features for certain models like logistic regression and SVM (tree-based models don't require it). Dealing with class imbalance: The dataset had roughly a 50/25/25 class split (home/draw/away). While training, we gave a slightly higher class weight to draws and away wins to ensure the model learns those minority classes. We also tracked not just overall accuracy but class-wise precision/recall. No significant missing data issues existed since every match had a full set of events (though if a match had incomplete data, we would drop it). However, some features could be zero for both teams (e.g., no dribbles in a match – very unlikely, but as an example). We handled such cases simply as zeros. Modeling Approach and Experiments We tried a variety of classification models, iteratively improving through experimentation: Baseline Guess and Majority Class: As a trivial baseline, always predicting "Home Win" would be right ~50% of the time. A slightly smarter baseline used the home win probability = 0.5, draw = 0.25, away = 0.25 (based on training distribution) and would get an expected accuracy around 42%. Our goal was to beat these baselines significantly.

1. Logistic Regression: We started with a multinomial logistic regression using all the engineered features. After one-hot encoding a few categorical and scaling, this model achieved about 55% accuracy on a validation set, with a decent precision for home wins but very poor recall for draws (it tended to rarely predict draw). We analyzed the coefficients: they made sense (e.g., home xG difference had a strongly positive weight for home win outcome). However, logistic regression was underfitting a complex pattern – likely too linear for interactions like "if both teams have high xG, it might be a draw" etc.

2. Random Forest: Next, we used a Random Forest classifier (sklearn) to capture non-linear interactions. We used 100 trees initially. This improved accuracy to ~60% on validation. The feature importance from the forest highlighted the xG difference, shots difference, and possession as top predictors, which matched our expectations. The RF did better on predicting draws than logistic (it picked up some patterns like "low total xG from both sides often leads to draws"). We performed hyperparameter tuning using GridSearchCV on number of trees and max_depth. We found an optimal around 300 trees, max_depth 5–7, beyond which we saw diminishing returns. The RF gave a balanced performance and became one of our strong models.  

3. Support Vector Machine (SVM): We trained an SVM with a linear kernel (one-vs- rest for 3 classes). Surprisingly, the linear SVM slightly outperformed the RF in raw accuracy (~65.8%) when allowed to converge fully. However, training was very slow with default settings because the linear SVM had no built-in iteration cap and was effectively trying to find the global optimum on a large feature set. We had to limit iterations to make it feasible, which dropped its accuracy to ~60%. The linear SVM's success hinted that a linear combination of features can do quite well if properly optimized, perhaps because our features already encapsulate non-linear aspects (like ratios). We also tried an RBF kernel SVM, but it was computationally expensive and didn't improve much.

4. XGBoost (Gradient Boosting): We applied XGBoost with softmax objective for multi-class. XGBoost achieved about 61-62% accuracy out of the box. It excelled in some cases but interestingly was a bit worse than Random Forest on draw predictions in initial runs. We suspect this is because boosting might have initially overfit the dominant home-win class. By tuning parameters (adding regularization, lowering learning rate, and ensuring sufficient trees), we got XGBoost to ~63% accuracy. The advantage of XGBoost is easier control over misclassification costs; we adjusted the scale_pos_weight  for draw class to emphasize it, which indeed improved draw recall slightly (though overall accuracy plateaued in mid-60s). During model evaluation, we paid attention to precision/recall for each outcome: For home wins, precision and recall were highest (around 0.7 and 0.8 in the better models) – models seldom miss a strong home favorite scenario. 

For draws, precision was low (<0.5) in many models – lots of false positives when predicting draw – and recall was also modest (~0.3-0.4). Draws are inherently the hardest to predict (often decided by random late goals etc.), which our results reflected. Away wins had intermediate performance; our models typically had slightly lower recall on away wins than home wins, which aligns with away wins being harder to identify. We performed cross-validation on seasons: e.g., train on 4 seasons, test on the next season. This approach simulates predicting future matches from past seasons. We noticed a slight dip in accuracy compared to random splits (since team strength evolves over seasons), but our models still performed ~60% in forward validation, indicating some stability. Ensembling Attempts: We experimented with combining models: An ensemble averaging RF, XGBoost, and logistic increased validation accuracy by about 1%. It helped particularly in capturing draws: one model might predict a draw where another predicted a team win, and averaging gave a probability that sometimes crossed the draw threshold. We also tried a simple stacking: using logistic regression on top of model probability outputs. This stacker learned to slightly upweight the cases where RF and XGB both gave moderate probability to draw, for example. The stacked model gave the best balanced accuracy (~64%). Notably, the SVM (when it converged well) was the single best model on accuracy, but due to its computational impracticality for tuning, we leaned on tree ensembles for the final pipeline. A critical part of experimentation was analyzing what didn't work:  

Including too many team-specific features caused overfit. A model might learn "Team A at home usually wins" which doesn't generalize. When we introduced team IDs as dummy variables, training accuracy jumped but test accuracy didn't improve. We removed these, relying on form metrics instead for team quality. We attempted to use some in-game odds (the betting odds provided in data) as features. While odds are highly predictive (bookmakers incorporate all info), using them made the problem trivial and less interesting (and in a real scenario you wouldn't include the thing you're trying to beat). So we omitted betting odds as features in final models, treating it as a pure predictive exercise. We tried a neural network (fully connected layers) on the features, but it didn't outperform tree models – likely due to limited data (only ~1500 matches for training after train-test split) and the structured nature of the problem which trees handled well. Model Performance and Evaluation Our final chosen model was an ensemble stack (Random Forest + XGBoost blended, with a logistic stacker). We evaluated it thoroughly: Overall Accuracy: ~64% on hold-out seasons. This is significantly better than the ~50% baseline and comparable to, though slightly below, some published studies (~65% reported). It means out of 100 EPL matches, our model predicts ~64 correctly. Confusion Matrix: We present the confusion matrix to understand how the model distributes its predictions: Confusion matrix of the final model on a validation set (rows: actual outcomes, columns: predicted). The model is fairly good at identifying home wins (40 out of 60 home wins predicted correctly) and away wins (30/50 correct). Draws are the toughest: only 20 of 40 actual draws were predicted, with many being misclassified as wins for one side. 

As seen above, the model most often confuses draws as wins or losses (e.g., it predicted "Home Win" in 10 cases that were actually draws, and "Away Win" in 10 cases that were draws). This aligns with our precision/recall findings: draw prediction had the lowest precision (only 20/ (15+20+15) ≈ 40% of predicted draws were correct in that example) and recall (50% of actual draws caught). Home wins had the highest precision and recall, as expected. Precision/Recall/F1: For home wins, precision ~0.68, recall ~0.75; for draws, precision ~0.40, recall ~0.50; for away wins, precision ~0.60, recall ~0.60. The macro-averaged F1 score was around 0.57. We also computed a weighted F1 (weighted by class frequency) ~0.62. These metrics highlight the imbalance: our model is very good at the common outcome (home win) and less so for rare outcomes. ROC and AUC: We plotted ROC curves for a one-vs-rest scenario (e.g., treating "home win" vs others). The AUC for home-win-vs-not was ~0.80, for away-win-vs-not ~0.75, and draw-vs-not ~0.67. This again reflected that separating draws from the rest is hardest. Calibration: We checked the probability calibration since we wanted to possibly use these predictions for betting simulations. The predicted probabilities for outcomes were reasonably calibrated – for instance, in matches where we predicted ~0.3 draw probability, about 30% ended up draws in aggregate. We used a softmax output from XGBoost and averaged with RF's predicted class probabilities, which gave us final probabilities.

In addition to accuracy, we considered real-world evaluation: would this model beat typical betting odds? We did a small simulation using historical odds – it turned out even a 64% accuracy model did not consistently generate profit in betting, reinforcing how difficult it is to outperform bookmakers (who incorporate more info and adjust odds). Our model tended to pick favorites (which odds reflect with low returns), and missed enough draws/upsets that a naive betting strategy wouldn't profit. This highlighted that a higher precision on underdog outcomes would be needed for a betting edge. Tools and Technologies Used Data retrieval and parsing: Python with json libraries to parse StatsBomb JSON files. We also used the pandas library extensively to manipulate event data into match-level summaries. The open-source statsbombpy package could retrieve data, but we mostly wrote custom parsing to tailor feature engineering. Feature Engineering: Many calculations were done with pandas DataFrames and Python logic. We also used NumPy for vectorized operations (e.g., computing xG totals). Modeling: We utilized scikit-learn for logistic regression, SVM, and RandomForest. For XGBoost, we used the xgboost Python library. We also tried lightgbm for a boosting alternative with similar results. Hyperparameter tuning was done via GridSearchCV and manual experimentation. Evaluation: scikit-learn provided methods to compute confusion matrices, classification reports (precision/recall), and ROC curves. We used matplotlib/seaborn to plot these, including the correlation heatmaps and feature importance charts. Environment: Jupyter notebooks facilitated interactive analysis. Computations were moderate; training a Random Forest with 300 trees or XGBoost with a few hundred rounds on ~1500 samples is quick (< a few seconds). The SVM was the slowest; we ran that on a more powerful machine with extended time. 

Version control: Git for code versioning, especially to keep track of feature engineering changes. We also documented each experiment's key results in notebook markdown or a log to avoid confusion. Conclusions and Lessons Learned Outcome: Our model achieved decent predictive power (~64% accuracy) on EPL outcomes, which is on par with other data-driven approaches in literature (most find it hard to exceed ~55-60% on average due to football's unpredictability). It underscored the challenge: even using detailed event data, football matches have many random factors (red cards, lucky goals, referee decisions) that make perfect prediction impossible. Key Learnings: Feature engineering is crucial in sports analytics: We spent a majority of time crafting features that encapsulate the game (xG, xT, possession, etc.). These features were far more predictive than raw stats (like just shots or passes alone). It taught us how domain knowledge (soccer tactics) can translate into model features. For example, the concept of "field tilt" and "expected threat" are domain-driven metrics that significantly improved the model once included. Handling class imbalance and evaluating beyond accuracy: We learned to pay attention to per-class performance. Initially we chased accuracy, but a model that predicts "home win" for every close call might get high accuracy but be useless for identifying draws. We adjusted our training (via class weights and threshold tuning) to ensure the model has at least some skill in predicting draws and away wins. This balanced approach is important in many business cases too (e.g., predicting rare events).

The danger of overfitting to teams or seasons: Our experiments with including team IDs or training and testing on the same season taught us about overfitting. A model that simply learned historical league standings might do well in sample but fail when team fortunes change (e.g., Leicester City's surprise title win would break a model relying on past standings!). We aimed for a model that relied on match metrics rather than team name, hopefully generalizing to new seasons and even other leagues. Model simplicity vs complexity: We found that simpler models like logistic regression and linear SVM weren't far off more complex ones, indicating a lot of the signal was linear. That was a surprising insight – perhaps because the engineered features themselves captured non-linear aspects. This reinforced that sometimes a well-framed feature set can allow even simple models to perform well (and they have the benefit of interpretability). On the other hand, ensemble methods gave us slight boosts and more flexibility in capturing interactions. Interpretability and trust: We valued interpretability; to present our findings, we often explained the model's decision for specific matches (e.g., "Model picked home win because home xG was much higher than away xG and possession was dominant"). These intuitive explanations helped stakeholders trust the model. Tree models and logistic coefficients were helpful here, as we could say which features pushed a prediction towards a certain outcome. Real-world perspective: Achieving 60+% accuracy doesn't guarantee success in applications like betting. We simulated betting and saw that without accounting for odds and risk, a predictive model alone isn't a golden ticket. It was a humbling lesson that in real business use, one must consider how the model's output will be used (e.g., making profitable bets requires not just saying who will win, but whether the odds offer value given the confidence).

Future Directions: We concluded that incorporating live data or lineup information could improve predictions (our model was pre-match only, without knowing the exact lineup or injuries). Also, using advanced algorithms like deep learning on sequence data (e.g., an LSTM over event sequences) might capture temporal patterns lost in aggregation. However, complexity grows and interpretability drops in those approaches. Another angle is using market odds as input in a careful way – perhaps to focus the model on games where odds and our model disagree, which might highlight value bets. All in all, our StatsBomb EPL outcome prediction project taught us a great deal about feature engineering, handling imbalanced classes, and the limits of predictability in sports. It strengthened our ability to blend domain knowledge with data science techniques, a skill transferrable to any project where understanding the context is as important as the algorithm.
