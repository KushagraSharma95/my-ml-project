---
title: "Census Income Classification"
summary: "Census Income Classification Problem Description and Business Context"
date: 2024-12-12
tags: [machine learning, portfolio]
---

# Census Income Classification
## Abstract
The task is to classify individuals' income levels (>50K or <=50K) using demographic and employment data from the UCI Adult dataset. The project addresses class imbalance and interprets influential features such as capital gain to understand income determinants.

## Introduction
Census Income Classification Problem Description and Business Context

The "Census Income" classification (also known as the Adult dataset) is a classic machine learning task. The objective is to predict whether an individual's annual income exceeds $50K or not, based on demographic and employment attributes. In a business context, a model like this could be used for marketing and customer segmentation – e.g., to identify potential high-income customers for premium products, or in public policy to understand income determinants. However, it also raises ethical considerations (possible biases) which we'll touch on. This is a binary classification problem: ">50K" versus "<=50K". The problem is interesting because the dataset contains a mix of numeric and categorical features, and it's known to have certain patterns (education, occupation, hours per week are strong predictors, while also reflecting societal biases). From a technical perspective, success means high classification accuracy and balanced performance (good recall on the minority class ">50K" since that's only ~25% of people). The baseline might be around 75% accuracy if one naively predicts everyone earns <=50K (because that's the majority class); a good model can reach ~85%-90% accuracy. But beyond accuracy, in a modern context, we also consider fairness – ensuring the model doesn't overly discriminate based on sensitive attributes like race or sex present in the data (since income is known to correlate with those historically). Data Source and Exploration We used the UCI Adult Census dataset (1994 Census data) which consists of: Instances: 48,842 individuals (32,561 in training set, 16,281 in test as provided by a common split).

## Data
Census Income Classification Problem Description and Business Context
The "Census Income" classification (also known as the Adult dataset) is a classic machine learning task. The objective is to predict whether an individual's annual income exceeds $50K or not, based on demographic and employment attributes. In a business context, a model like this could be used for marketing and customer segmentation – e.g., to identify potential high-income customers for premium products, or in public policy to understand income determinants. However, it also raises ethical considerations (possible biases) which we'll touch on. This is a binary classification problem: ">50K" versus "<=50K". The problem is interesting because the dataset contains a mix of numeric and categorical features, and it's known to have certain patterns (education, occupation, hours per week are strong predictors, while also reflecting societal biases). From a technical perspective, success means high classification accuracy and balanced performance (good recall on the minority class ">50K" since that's only ~25% of people). The baseline might be around 75% accuracy if one naively predicts everyone earns <=50K (because that's the majority class); a good model can reach ~85%-90% accuracy. But beyond accuracy, in a modern context, we also consider fairness – ensuring the model doesn't overly discriminate based on sensitive attributes like race or sex present in the data (since income is known to correlate with those historically). Data Source and Exploration We used the UCI Adult Census dataset (1994 Census data) which consists of: Instances: 48,842 individuals (32,561 in training set, 16,281 in test as provided by a common split).
Features: 14 attributes including age, workclass, education level, education num (continuous version of education), marital-status, occupation, relationship, race, sex, capital-gain, capital-loss, hours-per-week, native-country. Target: Income category (<=50K or >50K). The data was extracted by Barry Becker from census tapes; it's reasonably clean but has some quirks: Missing values are indicated by '?'. Specifically, workclass, occupation, and native-country have some missing entries (because some people didn't respond or it wasn't applicable). About 7% of records have missing values in those fields. The class distribution: approximately 76% have <=50K, 24% >50K – so the classes are imbalanced towards lower income. Initial Exploration: Numerical features stats: We found age ranges from 17 to 90 (with a mean around 38). Hours-per-week ranges 1 to 99 (mean ~40). Capital-gain and capital-loss are mostly zero for most people (with a small percentage having non-zero values; those features are highly skewed). Categorical breakdown: We looked at frequency of categories: workclass: e.g., "Private" is most common (~70%), then "Self-emp", "Government" etc., and a small number of "Never-worked" or "Without- pay". education: from "Preschool" to "Doctorate". We see a substantial number of HS-grad, Some-college, Bachelors, etc. marital-status: Married (jointly) vs Never-married are large chunks. occupation: 14 categories like Tech-support, Craft-repair, etc. race: ~85% White, ~10% Black, rest other (Asian-Pac-Islander, Amer- Indian-Eskimo, Other).
sex: about 2/3 male, 1/3 female in this dataset (reflecting the working adult subset). native-country: majority United-States (~90%), next common like Mexico, Philippines, Germany, etc., and many with "?" (missing). Target correlations: We did some quick pivot tables to see how income correlates: Education: As expected, higher education levels have higher proportion >50K. E.g., >50K rate for Doctorate was very high, while for HS-grad it's much lower. Marital-status: Married individuals (especially Married-civ-spouse) have a much higher chance of >50K than Never-married or divorced individuals (likely reflecting two incomes or one high earner). Occupation: Exec-managerial and Prof-specialty have high >50K rates; occupations like Other-service or Handlers-cleaners have very low >50K rates. Sex: Males in this data have a significantly higher >50K rate than females (this reflects known 1990s income disparities). Race: White and Asian-Pac-Islander have somewhat higher >50K rates than Black or Amer-Indian (again reflecting societal inequality). Hours: People working full-time (40+ hours) more likely >50K, and interestingly those with extremely high hours (>60) sometimes are high earners (though also potentially multiple job holders). We also plotted a histogram of age by income group: we see >50K group tends to concentrate in 30-50 age range (peak earning years), whereas <=50K group has more younger and older folks. This exploration already highlights potential biases (sex, race differences). It set a context that our model might leverage those unless we account for fairness. In a portfolio, one might mention being aware of this.
Data Preprocessing and Feature Engineering We performed several preprocessing steps: Handling missing values: We opted to remove records with missing workclass or occupation (since it was only ~7% of data and handling them specially wasn't critical for performance). Alternatively, one could treat "?" as its own category (like "Unknown"), which is also reasonable; in fact we tried that and it gave similar results. We did combine "Never-worked" and "Without- pay" in workclass into an "Other/None" category due to low counts. Categorical encoding: We used label encoding for binary categories like sex (Female=0, Male=1) and a few ordinal ones like education-num is already numeric. For nominal categories (workclass, education, marital-status, occupation, relationship, race, native-country), we employed one-hot encoding because we didn't want to impose an arbitrary ordinal relationship. This expanded our feature space. E.g., 8 workclass categories -> 8 dummy features, etc. We dropped one dummy from each to avoid multicollinearity (e.g., drop "Never-married" from marital so it's baseline). Feature engineering: We added a couple of features: Age buckets: We binned age into groups (like <30, 30-40, 40-50, 50-60, >60). Sometimes decision trees can learn this, but for linear models or just for exploration, buckets help. We suspected non-linear relationship (income peaks in middle age then maybe drops after retirement age). Capital gain/loss indicator: Since most people have zero capital gains/losses, we made a binary flag if they have any capital gain or any capital loss. And also kept the numeric amount. Hours per week buckets: Created a flag for those working very long hours (> 50) as it might indicate multiple jobs or high dedication, possibly affecting income.
Marital-status simplified: We made a simpler binary feature "IsMarried" (Married-civ-spouse True vs others False). Because married-civ-spouse stands out as high income group; others (never, divorced, separated) are typically lower. Relationship simplified: The "relationship" feature (Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried) overlaps with marital and sex. We one-hot encoded it, but also noted "Husband" and "Wife" basically indicates married male/female; "Own-child" often younger individuals in family. This feature turned out important in models because "Husband" corresponds to married male (often primary earner historically) which correlates to >50K. Scaling: For algorithms like SVM or logistic regression, we scaled continuous features (age, hours, capital gain/loss) to standard normal. For tree-based, scaling isn't needed, but we still did for uniformity in pipeline. Train-Test Split: We used the provided split (from UCI) for final evaluation to compare to known benchmarks. But during development, we often did cross- validation on the train set for hyperparameter tuning. After preprocessing, we had a feature matrix of about ~100 features (after one-hot expansions and our added features). The exact number depends on encoding (e.g., countries one-hot ~40 categories, occupation 14, etc.). Modeling and Experimentation We tried a range of models, from simple to more complex:
1. Logistic Regression (Baseline): We trained a logistic regression on the processed features. With class imbalance, we considered adding class weights (we ended up giving slightly higher weight to >50K class to improve recall). The logistic regression achieved about 85% accuracy on the test set, with an AUC ~0.92. It had a precision ~0.72 and recall ~0.55 for the >50K class (without heavy class weighting). With some weighting, we got recall up to ~0.60 at cost of precision. This baseline is already decent and interpretable: We looked at coefficients: e.g., higher education (like Bachelors, Masters dummies) had positive coefficients, being male had positive coef, being married had one of the largest positive coefs (married-civ-spouse ~2.5), hours- per-week had a positive coef, whereas being Female had a negative coef (indicating lower probability of >50K). This mirrors societal patterns but also underscores bias (our model could easily pick up on sex). This raised a question: do we want the model to use "sex" as a feature? For pure accuracy, yes it helps (because historically men earned more). But ethically, we might exclude it or account for fairness. As a demonstration, we trained one logistic model without sex and saw accuracy drop slightly (~1% or so) – meaning sex was predictive. We decided to keep it for baseline modeling but note this consideration.
3. Random Forest: Using an ensemble of 100 trees (depth up to 10, to fully use features but not overfit every quirk), we got accuracy ~86.5%. It improved recall for >50K to ~0.63 while keeping precision ~0.75. The forest's feature importance highlighted: age, capital- gain, hours, education-num, marital-status, relationship, and occupation as top predictors. We saw capital-gain shooting up in importance – indeed, having any capital gains often directly indicates investment income which correlates with >50K. We tuned number of trees and found beyond 100, marginal gains. Max_depth beyond 15 caused a tiny overfit.
5. SVM: We tried a support vector machine with RBF kernel. On the full dataset (~30k train), this was quite slow. We subsampled for tuning, and ultimately used an SVM with an RBF kernel, C=1, gamma=0.1 (after scaling features). It achieved around 85% accuracy – not better than tree-based or logistic, but respectable. SVM may not shine here because data isn't extremely high dimensional or with weird boundaries; plus the one-hots make the space large for RBF to handle. We ended up not using SVM further due to speed and no clear benefit.
6. Neural Network: A simple feedforward network (2 hidden layers with 50 neurons each) was trained. It got to ~86% accuracy. Tuning architecture didn't yield improvements beyond what boosting gave, likely because this dataset is not extremely large and structure is well handled by simpler models. NN could be overkill, and interpretability is worse. Model Selection: Overall, XGBoost or LightGBM gave the best performance around 87-88% accuracy, F1 about 0.70 for >50K class. However, the logistic regression was not far behind and was far more interpretable. For a portfolio, we might highlight that logistic regression already captures much of the signal and allows us to inspect biases. We also considered ensemble (blending logistic and trees), but the gain was minor (<0.5%). Given enough data, complex ensembles aren't needed; a single strong model suffices. Model Performance and Evaluation On the test set (which is the standard Adult.test from UCI with ~16k samples): Accuracy: Our best model (LightGBM) achieved about 87.5% accuracy. That means it misclassified roughly 12.5% of individuals. Many of those errors are likely on the borderline cases (people with incomes around 50K). Confusion Matrix: True <=50K: of ~12,000 actual <=50K, model correctly predicted ~11,000 and mistakenly put ~1,000 as >50K (false positives). True >50K: of ~4,000 actual >50K, model correctly predicted ~3,200 and missed ~800 (false negatives). So, the model catches about 80% of high-income people (sensitivity ~0.80), and when it says someone is high-income, it's correct about 76% of time (precision ~0.76). We can present these in a matrix or mention those rates. ROC AUC: ~0.94, indicating strong ranking ability. F1 Score: For >50K class ~0.73. This balances the ~0.80 recall and ~0.76 precision. We also calculated Log Loss for interest: around 0.32. Important Features: In the final boosted model, the rank of features by importance was:
XGBoost/LightGBM for gradient boosting models – using their Python APIs for faster training than sklearn's GradientBoosting. Matplotlib/Seaborn for visualizations: plotted distributions, bar charts of feature importances, ROC curves, etc. Jupyter Notebook to iterate on EDA and modeling. Possibly used GridSearchCV or random search from scikit-learn to tune hyperparameters for some models (like SVM C and gamma, random forest depth, etc.) using cross-validation on train data. We ensured to use proper train-test splits to evaluate unbiased performance. The dataset is not so large, so training was quick (seconds for logistic, a minute for random forest or boosting with tens of thousands of samples). Conclusion and Lessons Learned This Census Income Classification project demonstrated a standard supervised learning workflow: We explored the data and understood its distributions and quirks (like skewed capital gains, missing values) – crucial to treat data correctly. We preprocessed carefully: handling missing data, encoding categorical variables, and creating meaningful new features (which boosted model performance). We tried a suite of algorithms from simple (logistic) to complex (boosting, SVM) and saw that while complex models can squeeze out a bit more accuracy, simpler models aren't far behind on this well-behaved tabular data.
Interpreting models gave insights: It confirmed known patterns (education, occupation influence income) and also flagged the model picking up on biases (gender, marital status). We learned to balance accuracy with fairness considerations. For instance, one lesson was that including the "relationship" feature (like Husband/Wife) plus sex essentially lets the model infer gender+marital combos which could raise fairness concerns. In an applied setting, we might choose to exclude or adjust those features if using this model for decisions, to avoid codifying historical biases. Performance vs Complexity: We achieved ~87-88% accuracy with boosting. Could we improve it further? Perhaps marginally with more ensemble or calibrating thresholds. But we might face diminishing returns. Considering the simplicity of data, reaching ~90% is usually near the max reported by others. We realized that sometimes a well-tuned simpler model (like a regularized logistic or random forest) can perform nearly as well as heavy hyperparameter- tuned boosting – an insight into the bias-variance trade-off and the value of feature engineering. Communication: Since this is a common dataset, we focused on presenting results clearly: confusion matrix, and explaining what the model found important. We also reflected on possible deployment: e.g., a bank might use this model to flag high-income leads, but they must be careful about using protected attributes. It taught us to always think beyond raw metrics and consider real-world impact. In summary, our final recommended model might be a LightGBM classifier for highest accuracy, or a Logistic Regression for a more interpretable solution (with only ~2% lower accuracy). Both perform well in classifying incomes. This project enriched our experience in handling mixed data types, dealing with imbalance, and mindful modeling with ethical considerations. We validated that with proper preprocessing and model tuning, we can reliably predict income range and glean insights from the model about socioeconomic factors of income – a valuable exercise in data science.
Predicting Soccer Match Outcomes:A Data-Driven Approach | by Fion Ou… approach-df6bea2429ec Predicting Soccer Match Outcomes:A Data-Driven Approach | by Fion Ou… approach-df6bea2429ec Predicting Soccer Match Outcomes:A Data-Driven Approach | by Fion Ou… approach-df6bea2429ec Predicting Soccer Match Outcomes:A Data-Driven Approach | by Fion Ou… approach-df6bea2429ec Kaggle RSNA Pneumonia Detection Challenge Explained | by Sebastian N… explained-c140b19bf903 Kaggle RSNA Pneumonia Detection Challenge Explained | by Sebastian N… explained-c140b19bf903 Kaggle RSNA Pneumonia Detection Challenge Explained | by Sebastian N… explained-c140b19bf903 RSNA Pneumonia Detection Challenge (2018) | RSNA
YOLO Object Detection Walkthrough for the RSNA Pneumonia Detection … pneumonia-detection-challenge-123ec9a9adf2?gi=f66e899ec6ee YOLO Object Detection Walkthrough for the RSNA Pneumonia Detection … pneumonia-detection-challenge-123ec9a9adf2?gi=f66e899ec6ee Kaggle RSNA Pneumonia Detection Challenge Explained | by Sebastian N… explained-c140b19bf903 YOLO Object Detection Walkthrough for the RSNA Pneumonia Detection … pneumonia-detection-challenge-123ec9a9adf2?gi=f66e899ec6ee Kaggle RSNA Pneumonia Detection Challenge Explained | by Sebastian N… explained-c140b19bf903 Kaggle RSNA Pneumonia Detection Challenge Explained | by Sebastian N… explained-c140b19bf903 Kaggle RSNA Pneumonia Detection Challenge Explained | by Sebastian N… explained-c140b19bf903 Kaggle RSNA Pneumonia Detection Challenge Explained | by Sebastian N… explained-c140b19bf903 Santander Value Prediction Challenge | by DCT | Medium Santander Value Prediction Challenge | by DCT | Medium
Santander Value Prediction Challenge | by DCT | Medium Santander Value Prediction Challenge | by DCT | Medium Santander Value Prediction Challenge | by DCT | Medium Santander Value Prediction Challenge | by DCT | Medium Santander Value Prediction Challenge | by DCT | Medium Santander Value Prediction Challenge | by DCT | Medium Santander Value Prediction Challenge | by DCT | Medium Santander Value Prediction Challenge | by DCT | Medium Santander Value Prediction Challenge | by DCT | Medium Santander Value Prediction Challenge | by DCT | Medium Santander Value Prediction Challenge | by DCT | Medium Santander Value Prediction Challenge | by DCT | Medium
Santander Value Prediction Challenge | by DCT | Medium Santander Value Prediction Challenge | by DCT | Medium Santander Value Prediction Challenge | by DCT | Medium Santander Value Prediction Challenge | by DCT | Medium Santander Value Prediction Challenge | by DCT | Medium Santander Value Prediction Challenge | by DCT | Medium Santander Value Prediction Challenge | by DCT | Medium Santander Value Prediction Challenge | by DCT | Medium UCI Machine Learning Repository Adult income(train, test) dataset - Kaggle UCI Machine Learning Repository UCI Machine Learning Repository UCI Machine Learning Repository
UCI Machine Learning Repository UCI Machine Learning Repository UCI Machine Learning Repository All Sources docs.numer medium pure.ulster.ac rsna blog.goodaudience archive.ics.uci kaggle

## Methodology
4. Gradient Boosted Trees (XGBoost/LightGBM): We trained an XGBoost classifier. With default parameters, it got to ~87% accuracy. After tuning (max_depth=5, learning_rate=0.1, 200 rounds), we reached ~87.5% with an AUC ~0.94. LightGBM similarly got ~87-88%. These were among the best performers. They especially improved on tricky segments of data by handling interactions well. For instance, the model could capture that if "education=Masters and age>30 and hours>40" then high probability of >50K, etc. We did careful cross-validation to avoid overfitting the test since it's fixed and known.
9. Capital-Loss: Non-zero capital losses also indicated financial activity correlating with higher income (though not as strongly as gains).
10. Native-country_US: If not from US, slightly lower odds of >50K (immigrants possibly having lower earnings on average, except some countries like India had high earners in tech – but those are smaller numbers). We also did partial dependence plots to see impact: e.g., hours/week had an increasing PDP up to ~50 hours, then plateau; capital-gain had an almost step function – zero vs non-zero. Bias and Fairness Considerations: We checked disparate impact: The model might predict ">50K" for males far more often than for females. Indeed, our model's recall for females >50K was lower. This mirrors data – only ~11% of females in data have >50K vs ~30% of males. The model reproduces this gap, which is expected if it's maximizing accuracy. If deploying such a model, we'd be cautious about using "sex" or adjusting for fairness (maybe train separate models or add fairness constraint). Similarly for race, the model could be less accurate for minority groups due to fewer examples. We might consider removing sensitive attributes to see how much accuracy drops (we did for sex: removing it decreased accuracy ~0.3%). Removing race had negligible effect on accuracy because other factors cover for it (education, etc). Ultimately, for our portfolio, we note these concerns and perhaps demonstrate one approach like training without those attributes to produce a "fairer" model at slight cost of accuracy. Tools and Technologies Pandas for data loading, cleaning (like dealing with '?' values, converting them to NaN and dropping or filling). Scikit-learn for encoding (OneHotEncoder), model training (LogisticRegression, DecisionTree, RandomForest, SVC, etc.), and metrics (accuracy_score, classification_report).

## Experiments and Evaluation
2. Decision Tree: We grew a decision tree (CART). Without constraints, it can overfit. We pruned or limited depth to about 5. The tree gave ~85% accuracy as well. A small tree's rules were interpretable: e.g., the root split was on "marital-status_Married-civ-spouse", then perhaps "education-num > 12.5", etc. It's nice for explainability, but the single tree wasn't as accurate as ensembles could be (and possibly not as stable for unseen data beyond test set).
8. Sex_Male: The model did use sex – being male increased probability of >50K (reflecting the bias in data).

## References
- [1] UCI Adult Dataset. University of California, Irvine Machine Learning Repository.
- [2] Pedregosa, F. et al. Scikit-learn: Machine Learning in Python. JMLR 2011.

## Appendix
2. Marital-Status_Married: Being married (specifically in a civil spouse arrangement) strongly contributed to >50K, perhaps as a proxy for dual income or stability in career.
3. Education-Num: The numeric education level (which is correlated with degree attained) – higher means more likely >50K.
4. Hours-Per-Week: Those who work more hours tend to earn more (with diminishing returns).
5. Occupation_Exec-managerial: Certain occupations bump the probability.
6. Age: Important but non-linear – middle-aged were highest earners, younger lower, oldest slightly lower as some retire.
7. Relationship_Husband: If the individual is labeled as Husband in a family, that was a strong positive (ties in with male + married).
Citations Overview | Numerai Docs Overview | Numerai Docs One year participating in the Numerai Machine Learning competition: 7 le… learning-competition-7-lessons-learned-d540318bd2c7 One year participating in the Numerai Machine Learning competition: 7 le… learning-competition-7-lessons-learned-d540318bd2c7 Overview | Numerai Docs Overview | Numerai Docs One year participating in the Numerai Machine Learning competition: 7 le… learning-competition-7-lessons-learned-d540318bd2c7 One year participating in the Numerai Machine Learning competition: 7 le… learning-competition-7-lessons-learned-d540318bd2c7 One year participating in the Numerai Machine Learning competition: 7 le… learning-competition-7-lessons-learned-d540318bd2c7 One year participating in the Numerai Machine Learning competition: 7 le… learning-competition-7-lessons-learned-d540318bd2c7
One year participating in the Numerai Machine Learning competition: 7 le… learning-competition-7-lessons-learned-d540318bd2c7 Overview | Numerai Docs Overview | Numerai Docs One year participating in the Numerai Machine Learning competition: 7 le… learning-competition-7-lessons-learned-d540318bd2c7 One year participating in the Numerai Machine Learning competition: 7 le… learning-competition-7-lessons-learned-d540318bd2c7 One year participating in the Numerai Machine Learning competition: 7 le… learning-competition-7-lessons-learned-d540318bd2c7 One year participating in the Numerai Machine Learning competition: 7 le… learning-competition-7-lessons-learned-d540318bd2c7 One year participating in the Numerai Machine Learning competition: 7 le… learning-competition-7-lessons-learned-d540318bd2c7 One year participating in the Numerai Machine Learning competition: 7 le… learning-competition-7-lessons-learned-d540318bd2c7 Predicting Soccer Match Outcomes:A Data-Driven Approach | by Fion Ou… approach-df6bea2429ec Predicting Soccer Match Outcomes:A Data-Driven Approach | by Fion Ou…
Predicting Soccer Match Outcomes:A Data-Driven Approach | by Fion Ou… approach-df6bea2429ec Predicting Soccer Match Outcomes:A Data-Driven Approach | by Fion Ou… approach-df6bea2429ec Predicting Soccer Match Outcomes:A Data-Driven Approach | by Fion Ou… approach-df6bea2429ec Predicting Soccer Match Outcomes:A Data-Driven Approach | by Fion Ou… approach-df6bea2429ec [PDF] Predicting Football Match Outcomes Using Event Data and Machine…
